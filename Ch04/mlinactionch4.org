* decision tree
- if email address is my.com
  - true : boaring email
  - false : if contains 'hockey'
    - true : email from friends
    - false : spam. don't read

** information gain
- H = \Sigma p(x_i ) * log_2 p(x_i)
- the higher, the complexer
#+BEGIN_SRC python
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet: #the the number of unique elements and their occurance
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
#+END_SRC

** splitting the dataset
#+BEGIN_SRC python
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet
    
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0; bestFeature = -1
    for i in range(numFeatures):        #iterate over all the features
        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature
        uniqueVals = set(featList)       #get a set of unique values
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)     
        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy
        if (infoGain > bestInfoGain):       #compare this to the best gain so far
            bestInfoGain = infoGain         #if better than current best, set to best
            bestFeature = i
    return bestFeature                      #returns an integer
#+END_SRC

#+BEGIN_SRC 
>>> l
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> l[:1]
[0]
>>> l[1+1:]
[2, 3, 4, 5, 6, 7, 8, 9]
#+END_SRC

** build tree
#+BEGIN_SRC python
  def majorityCnt(classList):
      ''' return map or value'''
      classCount={}
      for vote in classList:
          if vote not in classCount.keys(): classCount[vote] = 0
          classCount[vote] += 1
      sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
      return sortedClassCount[0][0]

    def createTree(dataSet,labels):
        classList = [example[-1] for example in dataSet]
        if classList.count(classList[0]) == len(classList): 
            return classList[0]#stop splitting when all of the classes are equal
        if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet
            return majorityCnt(classList)
        bestFeat = chooseBestFeatureToSplit(dataSet)
        bestFeatLabel = labels[bestFeat]
        myTree = {bestFeatLabel:{}}
        del(labels[bestFeat])
        featValues = [example[bestFeat] for example in dataSet]
        uniqueVals = set(featValues)
        for value in uniqueVals:
            subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
        return myTree                            
#+END_SRC

#+BEGIN_SRC 
>>> l
[[1, 2], [3, 4], [5, 6]]
>>> len(l)
3
>>> [e[-1] for e in l]
[2, 4, 6]

>>> tree = trees.createTree(d,l)
>>> tree
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
#+END_SRC
** classify
#+BEGIN_SRC python
def classify(inputTree,featLabels,testVec):
    firstStr = inputTree.keys()[0]
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    key = testVec[featIndex]
    valueOfFeat = secondDict[key]
    if isinstance(valueOfFeat, dict): 
        classLabel = classify(valueOfFeat, featLabels, testVec)
    else: classLabel = valueOfFeat
    return classLabel
#+END_SRC

#+BEGIN_SRC 
>>> isinstance(l, list)
True
>>> type(l)
<type 'list'>
>>> type({})
<type 'dict'>
#+END_SRC

** load/save with pickle
load/save with pickle lib
#+BEGIN_SRC python
def storeTree(inputTree,filename):
    import pickle
    fw = open(filename,'w')
    pickle.dump(inputTree,fw)
    fw.close()
    
def grabTree(filename):
    import pickle
    fr = open(filename)
    return pickle.load(fr)
#+END_SRC
** example, contact lens type
#+BEGIN_SRC 
lenses = [inst.strip().split('\t') for inst in fr.readlines()]
>>> lenses
[['young', 'myope', 'no', 'reduced', 'no lenses'], ['young', 'myope', 'no', 'normal', 'soft'], ['young', 'myope', 'yes', 'reduced', 'no lenses'], ['young', 'myope', 'yes', 'normal', 'hard'], ['young', 'hyper', 'no', 'reduced', 'no lenses'], ['young', 'hyper', 'no', 'normal', 'soft'], ['young', 'hyper', 'yes', 'reduced', 'no lenses'], ['young', 'hyper', 'yes', 'normal', 'hard'], ['pre', 'myope', 'no', 'reduced', 'no lenses'], ['pre', 'myope', 'no', 'normal', 'soft'], ['pre', 'myope', 'yes', 'reduced', 'no lenses'], ['pre', 'myope', 'yes', 'normal', 'hard'], ['pre', 'hyper', 'no', 'reduced', 'no lenses'], ['pre', 'hyper', 'no', 'normal', 'soft'], ['pre', 'hyper', 'yes', 'reduced', 'no lenses'], ['pre', 'hyper', 'yes', 'normal', 'no lenses'], ['presbyopic', 'myope', 'no', 'reduced', 'no lenses'], ['presbyopic', 'myope', 'no', 'normal', 'no lenses'], ['presbyopic', 'myope', 'yes', 'reduced', 'no lenses'], ['presbyopic', 'myope', 'yes', 'normal', 'hard'], ['presbyopic', 'hyper', 'no', 'reduced', 'no lenses'], ['presbyopic', 'hyper', 'no', 'normal', 'soft'], ['presbyopic', 'hyper', 'yes', 'reduced', 'no lenses'], ['presbyopic', 'hyper', 'yes', 'normal', 'no lenses']]
>>> l = ['age', 'prescript', 'astigmatic', 'tearRate']
>>> import trees
>>> lenseTree = trees.createTree(lenses, l)
>>> lenseTree
{'tearRate': {'reduced': 'no lenses', 'normal': {'astigmatic': {'yes': {'prescript': {'hyper': {'age': {'pre': 'no lenses', 'presbyopic': 'no lenses', 'young': 'hard'}}, 'myope': 'hard'}}, 'no': {'age': {'pre': 'soft', 'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}}, 'young': 'soft'}}}}}}
#+END_SRC
** summary 
- 너무 많은 분할이 생길경우 문제
- 수치형 값을 다룰수 없음

* naive bayes
** conditional probability
P(A|B) = P(A&B)/P(B)
| bucket A | bucket B |
|----------+----------|
| G G B B  | G B B    |
|----------+----------|


P(G|bucketB) = P( gray & bucketB) / P(bucketB) = 1/7/(3/7) = 1/3

- p1(c_1 |x,y) > p2(c_2 |x,y)  -> class 1
- p1(c_1 |x,y) < p2(c_2 |x,y)  -> class 2
** classifying text
*** making word vectors from text
set : buil-in data structure for set
#+BEGIN_SRC python
def createVocabList(dataSet):
    vocabSet = set([])  #create empty set
    for document in dataSet:
        vocabSet = vocabSet | set(document) #union of the two sets
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else: print "the word: %s is not in my Vocabulary!" % word
    return returnVec
#+END_SRC
bayes.setOfWords2Vec(my, p[0])
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]
*** training
p(c_i | w) = p(w | c_i )* p(c_i)/p(w)
- use log(x) to avoid 0 multiplier, underflow
- p(w) 는 무시할 수 있다. p(0), p(1)도 둘다 공통으로 가지는 속성이니까.

#+BEGIN_SRC python
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() 
    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = log(p1Num/p1Denom)          #change to log()
    p0Vect = log(p0Num/p0Denom)          #change to log()
    return p0Vect,p1Vect,pAbusive
#+END_SRC

*** test
#+BEGIN_SRC python
  def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
      p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult
      p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
      if p1 > p0:
          return 1
      else: 
          return 0

  def testingNB():
      listOPosts,listClasses = loadDataSet()
      myVocabList = createVocabList(listOPosts)
      trainMat=[]
      for postinDoc in listOPosts:
          trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
      p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))
      testEntry = ['love', 'my', 'dalmation']
      thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
      print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
      testEntry = ['stupid', 'garbage']
      thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
      print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
#+END_SRC

*** bag-of-words document model
기존에는 0/1 이었지만 이제는 0---N까지 생길 수 있음
#+BEGIN_SRC python
def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec
#+END_SRC

** classifying spam emails

#+BEGIN_SRC python
  def textParse(bigString):    #input is big string, #output is word list
      import re
      listOfTokens = re.split(r'\W*', bigString) # \\W means non-word char
      return [tok.lower() for tok in listOfTokens if len(tok) > 2] 
      
  def spamTest():
      docList=[]; classList = []; fullText =[]
      for i in range(1,26):
          wordList = textParse(open('email/spam/%d.txt' % i).read())
          docList.append(wordList)
          fullText.extend(wordList)
          classList.append(1)
          wordList = textParse(open('email/ham/%d.txt' % i).read())
          docList.append(wordList)
          fullText.extend(wordList)
          classList.append(0)
      vocabList = createVocabList(docList)#create vocabulary
      trainingSet = range(50); testSet=[]           #create test set
      for i in range(10):
          randIndex = int(random.uniform(0,len(trainingSet)))
          testSet.append(trainingSet[randIndex])
          del(trainingSet[randIndex])  
      trainMat=[]; trainClasses = []
      for docIndex in trainingSet:#train the classifier (get probs) trainNB0
          trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))
          trainClasses.append(classList[docIndex])
      p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))
      errorCount = 0
      for docIndex in testSet:        #classify the remaining items
          wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
          if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:
              errorCount += 1
              print "classification error",docList[docIndex]
      print 'the error rate is: ',float(errorCount)/len(testSet)
#+END_SRC

** rss feed
lambda
#+BEGIN_SRC 
>>> l
[(1, 0.1), (2, 0.01)]
>>> sorted(l, key=lambda p:p[1])
[(2, 0.01), (1, 0.1)]
>>> sorted(l, key=lambda p:p[0])
[(1, 0.1), (2, 0.01)]
#+END_SRC

#+BEGIN_SRC python
  def calcMostFreq(vocabList,fullText):
      import operator
      freqDict = {}
      for token in vocabList:
          freqDict[token]=fullText.count(token)
      sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) 
      return sortedFreq[:30]       

  def localWords(feed1,feed0):
      import feedparser
      docList=[]; classList = []; fullText =[]
      minLen = min(len(feed1['entries']),len(feed0['entries']))
      for i in range(minLen):
          wordList = textParse(feed1['entries'][i]['summary'])
          docList.append(wordList)
          fullText.extend(wordList)
          classList.append(1) #NY is class 1
          wordList = textParse(feed0['entries'][i]['summary'])
          docList.append(wordList)
          fullText.extend(wordList)
          classList.append(0)
      vocabList = createVocabList(docList)#create vocabulary
      top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words 'stop words'
      for pairW in top30Words:
          if pairW[0] in vocabList: vocabList.remove(pairW[0])
      trainingSet = range(2*minLen); testSet=[]           #create test set
      for i in range(20):
          randIndex = int(random.uniform(0,len(trainingSet)))
          testSet.append(trainingSet[randIndex])
          del(trainingSet[randIndex])  
      trainMat=[]; trainClasses = []
      for docIndex in trainingSet:#train the classifier (get probs) trainNB0
          trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))
          trainClasses.append(classList[docIndex])
      p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))
      errorCount = 0
      for docIndex in testSet:        #classify the remaining items
          wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
          if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:
              errorCount += 1
      print 'the error rate is: ',float(errorCount)/len(testSet)
      return vocabList,p0V,p1V


  def getTopWords(ny,sf):
      ''' print frequent words by probability'''
      import operator
      vocabList,p0V,p1V=localWords(ny,sf)
      topNY=[]; topSF=[]
      for i in range(len(p0V)):
          if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))
          if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))
      sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)
      print "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**"
      for item in sortedSF:
          print item[0]
      sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)
      print "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**"
      for item in sortedNY:
          print item[0]
#+END_SRC


